import sounddevice as sd
import numpy as np
from openai import OpenAI
import queue
import threading
import time
import os
import sys
import tempfile
import soundfile as sf
from collections import deque
from dotenv import load_dotenv
import asyncio
import logging
from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack
from aiortc.contrib.media import MediaRelay
import whisper

load_dotenv()

os.environ["OPEN_API_KEY"] = os.getenv("OPENAI_API_KEY")

# OpenAI Whisper API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = OpenAI()

# í ì„¤ì •
audio_queue = queue.Queue()

# ì˜¤ë””ì˜¤ ì„¤ì •
samplerate = 16000
block_size = 4000  # 0.25ì´ˆ ë¶„ëŸ‰

# ìë§‰ ê´€ë¦¬ë¥¼ ìœ„í•œ ì„¤ì •
caption_history = deque(maxlen=5)  # ìµœê·¼ 5ê°œ ë¬¸ì¥ ì €ì¥
current_caption = ""
caption_lock = threading.Lock()

# Logging ì„¤ì •
logging.basicConfig(level=logging.INFO)

# Whisper ëª¨ë¸ ë¡œë“œ
model = whisper.load_model("base")

# WebRTC ì—°ê²°ì„ ìœ„í•œ PeerConnection
pcs = set()
relay = MediaRelay()

class AudioTrack(MediaStreamTrack):
    """
    í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ìˆ˜ì‹ ëœ ì˜¤ë””ì˜¤ íŠ¸ë™ì„ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    """
    kind = "audio"

    def __init__(self, track):
        super().__init__()  # MediaStreamTrack ì´ˆê¸°í™”
        self.track = track

    async def recv(self):
        frame = await self.track.recv()

        # Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± ì¸ì‹ ì²˜ë¦¬
        audio_data = frame.to_ndarray()
        result = model.transcribe(audio_data)
        logging.info(f"Transcription: {result['text']}")

        return frame

# ì˜¤ë””ì˜¤ ì½œë°±
def audio_callback(indata, frames, time, status):
    if status:
        print(f"ìƒíƒœ: {status}", file=sys.stderr)
    audio_queue.put(indata.copy())

# í™”ë©´ ì§€ìš°ê¸° í•¨ìˆ˜
def clear_screen():
    os.system('cls' if os.name == 'nt' else 'clear')

# ìë§‰ ì¶œë ¥ í•¨ìˆ˜
def update_captions():
    clear_screen()
    print("\n\n\n")
    print("=" * 60)
    print("ğŸ™ï¸ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ìë§‰ (Ctrl+Cë¡œ ì¢…ë£Œ)")
    print("=" * 60)

    for prev in list(caption_history)[:-1]:
        print(f"\033[90m{prev}\033[0m")

    if caption_history:
        print(list(caption_history)[-1])

    if current_caption:
        print(f"\033[1m{current_caption}\033[0m", end="â–‹\n")
    else:
        print("â–‹")
    print("=" * 60)

# ì˜¤ë””ì˜¤ ìˆ˜ì§‘ ìŠ¤ë ˆë“œ
def audio_collection_thread():
    try:
        with sd.InputStream(samplerate=samplerate, channels=1, 
                          callback=audio_callback, blocksize=block_size):
            print("ğŸ™ï¸ ì‹¤ì‹œê°„ STT ì‹œì‘ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
            while True:
                time.sleep(0.1)
    except Exception as e:
        print(f"ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {e}", file=sys.stderr)
    except KeyboardInterrupt:
        pass

# STT ì²˜ë¦¬ ìŠ¤ë ˆë“œ (OpenAI Whisper API ìµœì‹  ë²„ì „)
def stt_processing_thread():
    global current_caption
    buffer = np.zeros((0, 1), dtype=np.float32)
    max_buffer_size = samplerate * 5

    try:
        while True:
            try:
                data = audio_queue.get(timeout=1)
                buffer = np.concatenate((buffer, data), axis=0)

                if len(buffer) > max_buffer_size:
                    buffer = buffer[-max_buffer_size:]

                chunk_size = int(samplerate * 3.0)
                if len(buffer) >= chunk_size:
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                        sf.write(f.name, buffer[:chunk_size], samplerate)
                        audio_file = open(f.name, "rb")
                        response = client.audio.transcriptions.create(
                            model="whisper-1",
                            file=audio_file,
                            language="ko"
                        ,
                            prompt="íšŒì˜ ì¤‘ì…ë‹ˆë‹¤. ë˜ë°•ë˜ë°• ë§í•˜ëŠ” ë‚´ìš©ì„ ë°›ì•„ì ì–´.")
                        audio_file.close()
                        os.unlink(f.name)

                    text = response.text.strip()
                    if text:
                        with caption_lock:
                            if not current_caption or text[0].isupper() or any(current_caption.endswith(p) for p in ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ']):
                                if current_caption:
                                    caption_history.append(current_caption)
                                current_caption = text
                            else:
                                current_caption += " " + text
                        update_captions()
                        buffer = np.zeros((0, 1), dtype=np.float32)

                audio_queue.task_done()
            except queue.Empty:
                continue
    except KeyboardInterrupt:
        pass

async def offer(request):
    """
    í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° SDP Offerë¥¼ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ë°˜í™˜
    """
    params = await request.json()
    offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])

    pc = RTCPeerConnection()
    pcs.add(pc)

    @pc.on("track")
    async def on_track(track):
        logging.info(f"Track {track.kind} received")
        if track.kind == "audio":
            pc.addTrack(AudioTrack(relay.subscribe(track)))

    await pc.setRemoteDescription(offer)
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    return {
        "sdp": pc.localDescription.sdp,
        "type": pc.localDescription.type
    }

async def cleanup():
    """
    WebRTC ì—°ê²° ì •ë¦¬
    """
    while True:
        await asyncio.sleep(10)
        for pc in pcs:
            if pc.connectionState == "closed":
                pcs.discard(pc)

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    try:
        clear_screen()

        t1 = threading.Thread(target=audio_collection_thread)
        t2 = threading.Thread(target=stt_processing_thread)

        t1.daemon = True
        t2.daemon = True

        t1.start()
        t2.start()

        update_captions()

        while True:
            time.sleep(0.1)

    except KeyboardInterrupt:
        clear_screen()
        print("\nğŸ›‘ í”„ë¡œê·¸ë¨ ì¢…ë£Œ...")
        time.sleep(0.5)
        print("ğŸ‘‹ ì¢…ë£Œ ì™„ë£Œ")
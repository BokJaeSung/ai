# Product Requirements Document (PRD)

## 프로젝트 개요

- **목적**: 클라이언트와 서버 간 WebRTC를 통해 오디오 데이터를 전송하고, 서버에서 Whisper를 사용하여 음성을 텍스트로 변환하는 시스템을 구축합니다.

---

## 1. 목표
### 1.1 주요 목표
- 클라이언트와 서버 간 실시간 오디오 스트리밍을 지원하는 WebRTC 연결 구현
- 서버에서 Whisper 모델을 사용하여 음성 데이터를 텍스트로 변환
- 변환된 텍스트를 클라이언트로 반환

### 1.2 성공 기준
- 클라이언트가 WebRTC를 통해 서버에 오디오 데이터를 전송할 수 있음
- 서버가 Whisper를 사용하여 음성을 정확히 텍스트로 변환
- 변환된 텍스트가 클라이언트로 반환됨

---

## 2. 기능 요구사항
### 2.1 WebRTC 서버
- 클라이언트로부터 SDP Offer를 수신하고 SDP Answer를 반환
- 오디오 스트림을 수신하고 Whisper 모델에 전달
- Whisper 결과를 클라이언트로 반환

### 2.2 Whisper 음성 인식
- Whisper 모델을 사용하여 오디오 데이터를 텍스트로 변환
- 다양한 언어를 지원하며, 기본적으로 영어를 처리

### 2.3 클라이언트-서버 통신
- WebRTC를 통해 오디오 데이터를 스트리밍
- FastAPI를 사용하여 HTTP 엔드포인트(`/offer`) 제공

---

## 3. 비기능 요구사항
### 3.1 성능
- 서버는 초당 최소 10개의 오디오 스트림을 처리할 수 있어야 함
- Whisper 모델은 1초 이내에 텍스트 변환을 완료해야 함

### 3.2 확장성
- 다수의 클라이언트가 동시에 연결 가능해야 함
- 필요 시 Whisper 모델을 GPU 환경에서 실행 가능

### 3.3 보안
- WebRTC 연결은 암호화된 채널을 통해 이루어져야 함
- Whisper 결과는 인증된 클라이언트에게만 반환

---

## 4. 기술 스택
- **프로그래밍 언어**: Python 3.11
- **프레임워크**: FastAPI
- **라이브러리**: aiortc, whisper
- **모델**: OpenAI Whisper (Base)
- **배포 환경**: 로컬 서버 또는 클라우드 환경 (AWS, Azure 등)

---

## 5. 시스템 아키텍처
### 5.1 구성 요소
1. **클라이언트**
   - WebRTC를 통해 오디오 데이터를 서버로 전송
   - 변환된 텍스트를 수신 및 표시
2. **서버**
   - WebRTC 연결 관리
   - Whisper를 사용하여 음성 인식 수행

### 5.2 데이터 흐름
1. 클라이언트가 SDP Offer를 서버로 전송
2. 서버가 SDP Answer를 반환하여 WebRTC 연결 설정
3. 클라이언트가 오디오 데이터를 스트리밍
4. 서버가 Whisper를 사용하여 음성을 텍스트로 변환
5. 변환된 텍스트를 클라이언트로 반환

---

## 6. 일정
| 단계                | 기간          | 주요 작업                          |
|---------------------|---------------|-----------------------------------|
| 요구사항 분석        | 2026-02-01   | PRD 작성 및 요구사항 정의          |
| 개발 환경 설정       | 2026-02-02   | 라이브러리 설치 및 초기 설정       |
| WebRTC 서버 개발     | 2026-02-03 ~ 2026-02-05 | WebRTC 연결 구현 및 테스트 |
| Whisper 통합         | 2026-02-06 ~ 2026-02-07 | 음성 인식 기능 구현 및 테스트 |
| 통합 테스트          | 2026-02-08   | 클라이언트-서버 통합 테스트        |
| 배포 및 문서화       | 2026-02-09   | 배포 및 최종 문서화                |

---

## 7. 위험 요소
- **WebRTC 연결 문제**: 네트워크 환경에 따라 연결이 불안정할 수 있음
- **Whisper 성능 문제**: 긴 오디오 데이터 처리 시 지연 발생 가능
- **확장성 문제**: 다수의 클라이언트 연결 시 서버 과부하 가능

---

## 8. 참고 자료
- [aiortc 공식 문서](https://aiortc.readthedocs.io/)
- [OpenAI Whisper GitHub](https://github.com/openai/whisper)
- [FastAPI 공식 문서](https://fastapi.tiangolo.com/)

---

## 9. 음성 합성 (TTS) 파이프라인 추가

### 9.1 개요
- AI 면접관이 Whisper로 변환된 텍스트를 기반으로 음성을 생성하여 지원자에게 질문을 전달합니다.
- TTS(Text-to-Speech) 엔진을 사용하여 텍스트를 음성으로 변환합니다.

### 9.2 파이프라인 흐름
1. **질문 생성**:
   - `interviewer_prompt.py` 파일에서 AI 면접관의 질문 프롬프트를 가져옵니다.
   - 질문은 사전에 정의된 템플릿을 기반으로 생성됩니다.

2. **TTS 변환**:
   - 생성된 질문 텍스트를 TTS 엔진에 전달하여 음성 파일로 변환합니다.
   - 변환된 음성 파일은 클라이언트로 스트리밍됩니다.

3. **클라이언트 전달**:
   - WebRTC를 통해 음성 데이터를 클라이언트로 전송합니다.

### 9.3 기술 스택
- **TTS 엔진**: Google TTS, pyttsx3 또는 기타 오픈소스 TTS 라이브러리
- **WebRTC**: aiortc를 사용하여 음성 스트리밍

### 9.4 구현 계획
- `interviewer_prompt.py` 파일에서 질문 텍스트를 생성
- TTS 엔진을 통합하여 텍스트를 음성으로 변환
- WebRTC 서버를 확장하여 음성 데이터를 클라이언트로 전송

---

**작성 완료**